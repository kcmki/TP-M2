{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\mekki\\AppData\\Local\\Temp\\ipykernel_7340\\1234238284.py:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n",
      "C:\\Users\\mekki\\AppData\\Local\\Temp\\ipykernel_7340\\1234238284.py:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n",
      "C:\\Users\\mekki\\AppData\\Local\\Temp\\ipykernel_7340\\1234238284.py:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n"
     ]
    }
   ],
   "source": [
    "class vect:\n",
    "    SCALAR = 1\n",
    "    COSINE = 2\n",
    "    JACCARD = 3\n",
    "\n",
    "class Docs:\n",
    "\n",
    "    def __init__(self,Lancaster=True,split=False):\n",
    "\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "        self.files = [\"TP2data/D1.txt\",\"TP2data/D2.txt\",\"TP2data/D3.txt\",\"TP2data/D4.txt\",\"TP2data/D5.txt\",\"TP2data/D6.txt\"]\n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.files)):\n",
    "            file = open(self.files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "\n",
    "            if not split :\n",
    "                Termes = file.split()\n",
    "            if split:\n",
    "\n",
    "                ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "                Termes = ExpReg.tokenize(file) \n",
    "\n",
    "\n",
    "            if Lancaster :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                Lancaster = nltk.LancasterStemmer()\n",
    "                TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "            else :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                porter = nltk.PorterStemmer()\n",
    "                TermesNormalisation = [porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term,i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "    \n",
    "    def rebuild(self,Lancaster=True,split=False):\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "        \n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.files)):\n",
    "            file = open(self.files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "            if not split :\n",
    "                Termes = file.split()\n",
    "            if split:\n",
    "                ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "                Termes = ExpReg.tokenize(file) \n",
    "\n",
    "\n",
    "            if Lancaster :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                Lancaster = nltk.LancasterStemmer()\n",
    "                TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "            else :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                porter = nltk.PorterStemmer()\n",
    "                TermesNormalisation = [porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term.lower(),i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "        self.DictWeight()\n",
    "    def normalize(self,text):\n",
    "        if self.Lancaster :\n",
    "            Lancaster = nltk.LancasterStemmer()\n",
    "            text = Lancaster.stem(text)\n",
    "        else :\n",
    "            porter = nltk.PorterStemmer()\n",
    "            text = porter.stem(text)\n",
    "        return text\n",
    "    def splitter(self,text):\n",
    "        if self.split:\n",
    "            return text.split(\" \")\n",
    "        else:\n",
    "            ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "            Termes = ExpReg.tokenize(text) \n",
    "            return Termes\n",
    "    def calcWeight(self,word,doc):\n",
    "\n",
    "        inDocs = []\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[0] == word:\n",
    "                if keys[1] not in inDocs:\n",
    "                    inDocs.append(keys[1])\n",
    "\n",
    "        # nombre de documents contenant le mot\n",
    "        Ni = len(inDocs)\n",
    "\n",
    "        try:\n",
    "            tf = self.dict[(word,doc)]/self.maxfreq[doc]\n",
    "        except:\n",
    "            tf = 0\n",
    "            \n",
    "        #idf = math.log10((N/Ni)+1)\n",
    "        try:\n",
    "            idf = math.log10((self.N/Ni)+1)\n",
    "        except:\n",
    "            idf = 0\n",
    "\n",
    "        return(tf*idf)\n",
    "\n",
    "    def DictWeight(self):\n",
    "\n",
    "        self.TDFP = [(word,doc,self.dict[(word,doc)],self.calcWeight(word,doc)) for word,doc in self.dict.keys()]\n",
    "        self.DTFP = [(doc,word,freq,weight) for word,doc,freq,weight in self.TDFP]\n",
    "        self.TDFP = sorted(self.TDFP, key=lambda x: x[0])\n",
    "\n",
    "    def MV(self,termes,typeV = vect.SCALAR):\n",
    "        rsv = {}\n",
    "        termes = self.splitter(termes)\n",
    "        normterms = []\n",
    "        for term in termes:\n",
    "            normterms.append(self.normalize(term))\n",
    "\n",
    "        for doc in range(self.N):\n",
    "            QD = 0\n",
    "            v2 = 0\n",
    "            w2 = 0\n",
    "            for term in normterms:\n",
    "                if (term.lower(),doc) in self.dict.keys():\n",
    "                    weight = self.calcWeight(term,doc)\n",
    "                    QD+= weight\n",
    "                    w2+= math.pow(weight,2)\n",
    "                    v2+=1\n",
    "            print(w2,v2)\n",
    "            try:\n",
    "                if typeV == vect.COSINE:\n",
    "                    QD = QD / (v2*w2)\n",
    "                if typeV == vect.JACCARD:\n",
    "                    QD = QD / (w2+v2 - QD)\n",
    "            except ZeroDivisionError:\n",
    "                QD = 0\n",
    "\n",
    "            rsv[doc] = QD\n",
    "            \n",
    "        return rsv\n",
    "\n",
    "        \n",
    "d = Docs(Lancaster=False,split=True)\n",
    "\n",
    "d.DictWeight()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0.08977205194893534 1\n",
      "0.01896306064961095 2\n",
      "0.10158604251801275 2\n",
      "0.08137635711861467 2\n",
      "0.009111059267549354 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0.3791925699343096,\n",
       " 2: 0.09677848750518504,\n",
       " 3: 0.22471246845697213,\n",
       " 4: 0.20460462614032912,\n",
       " 5: 0.07181190846697223}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.MV(\"documents ranking\",typeV=vect.JACCARD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
