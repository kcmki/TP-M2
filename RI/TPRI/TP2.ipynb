{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_13044\\2324064935.py:26: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n",
      "C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_13044\\2324064935.py:77: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n",
      "C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_13044\\2324064935.py:123: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n"
     ]
    }
   ],
   "source": [
    "class vect:\n",
    "    SCALAR = 1\n",
    "    COSINE = 2\n",
    "    JACCARD = 3\n",
    "\n",
    "class Docs:\n",
    "\n",
    "    def __init__(self,Lancaster=True,split=False):\n",
    "\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "        self.files = [\"TP2data/D1.txt\",\"TP2data/D2.txt\",\"TP2data/D3.txt\",\"TP2data/D4.txt\",\"TP2data/D5.txt\",\"TP2data/D6.txt\"]\n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.files)):\n",
    "            file = open(self.files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "\n",
    "            if not split :\n",
    "                Termes = file.split()\n",
    "            if split:\n",
    "\n",
    "                ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "                Termes = ExpReg.tokenize(file) \n",
    "\n",
    "\n",
    "            if Lancaster :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                Lancaster = nltk.LancasterStemmer()\n",
    "                TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "            else :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                porter = nltk.PorterStemmer()\n",
    "                TermesNormalisation = [porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term,i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "    \n",
    "    def rebuild(self,Lancaster=True,split=False):\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "        \n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.files)):\n",
    "            file = open(self.files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "\n",
    "\n",
    "            if not split :\n",
    "                Termes = file.split()\n",
    "            if split:\n",
    "                ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "                Termes = ExpReg.tokenize(file) \n",
    "\n",
    "\n",
    "            if Lancaster :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                Lancaster = nltk.LancasterStemmer()\n",
    "                TermesNormalisation = [Lancaster.stem(terme) for terme in TermesSansMotsVides]\n",
    "            else :\n",
    "                TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "                porter = nltk.PorterStemmer()\n",
    "                TermesNormalisation = [porter.stem(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term.lower(),i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "        self.DictWeight()\n",
    "    def normalize(self,text):\n",
    "        if self.Lancaster :\n",
    "            Lancaster = nltk.LancasterStemmer()\n",
    "            text = Lancaster.stem(text)\n",
    "        else :\n",
    "            porter = nltk.PorterStemmer()\n",
    "            text = porter.stem(text)\n",
    "        return text\n",
    "    def splitter(self,text):\n",
    "        if not self.split:\n",
    "            return text.split(\" \")\n",
    "        else:\n",
    "            ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "            Termes = ExpReg.tokenize(text) \n",
    "            return Termes\n",
    "    def calcWeight(self,word,doc):\n",
    "\n",
    "        inDocs = []\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[0] == word:\n",
    "                if keys[1] not in inDocs:\n",
    "                    inDocs.append(keys[1])\n",
    "\n",
    "        # nombre de documents contenant le mot\n",
    "        Ni = len(inDocs)\n",
    "\n",
    "        try:\n",
    "            tf = self.dict[(word,doc)]/self.maxfreq[doc]\n",
    "        except:\n",
    "            tf = 0\n",
    "            \n",
    "        #idf = math.log10((N/Ni)+1)\n",
    "        try:\n",
    "            idf = math.log10((self.N/Ni)+1)\n",
    "        except:\n",
    "            idf = 0\n",
    "\n",
    "        return(tf*idf)\n",
    "\n",
    "    def DictWeight(self):\n",
    "\n",
    "        self.TDFP = [(word,doc,self.dict[(word,doc)],self.calcWeight(word,doc)) for word,doc in self.dict.keys()]\n",
    "        self.DTFP = [(doc,word,freq,weight) for word,doc,freq,weight in self.TDFP]\n",
    "        self.TDFP = sorted(self.TDFP, key=lambda x: x[0])\n",
    "\n",
    "    def MV(self,termes,typeV = vect.SCALAR):\n",
    "        rsv = {}\n",
    "        termes = self.splitter(termes)\n",
    "        normterms = []\n",
    "        for term in termes:\n",
    "            normterms.append(self.normalize(term))\n",
    "\n",
    "        for doc in range(self.N):\n",
    "            QD = 0\n",
    "            v2 = 0\n",
    "            w2 = 0\n",
    "            for term in normterms:\n",
    "                if (term.lower(),doc) in self.dict.keys():\n",
    "                    weight = self.calcWeight(term,doc)\n",
    "                    QD+= weight\n",
    "                    w2+= math.pow(weight,2)\n",
    "                    v2+=1\n",
    "            try:\n",
    "                if typeV == vect.COSINE:\n",
    "                    QD = QD / (v2*w2)\n",
    "                if typeV == vect.JACCARD:\n",
    "                    QD = QD / (w2+v2 - QD)\n",
    "            except ZeroDivisionError:\n",
    "                QD = 0\n",
    "\n",
    "\n",
    "            rsv[doc] = QD\n",
    "\n",
    "        rsv = dict(sorted(rsv.items(), key=lambda item: item[1],reverse=True))\n",
    "        return rsv\n",
    "\n",
    "        \n",
    "d = Docs(Lancaster=False,split=True)\n",
    "\n",
    "d.DictWeight()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_13044\\1199361307.py\", line 37, in on_token_change\n",
      "    search()\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_13044\\1199361307.py\", line 10, in search\n",
      "    table.delete(*table.get_children())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\ttk.py\", line 1195, in get_children\n",
      "    self.tk.call(self._w, \"children\", item or '') or ())\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "_tkinter.TclError: invalid command name \".!frame.!treeview\"\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "def search():\n",
    "    global stemming\n",
    "\n",
    "    Lancaster=stemming.get()\n",
    "\n",
    "    text = query_text.get()\n",
    "    table.delete(*table.get_children())\n",
    "    \n",
    "    if Lancaster :\n",
    "        Lancaster = nltk.LancasterStemmer()\n",
    "        text = Lancaster.stem(text)\n",
    "    else :\n",
    "        porter = nltk.PorterStemmer()\n",
    "        text = porter.stem(text)\n",
    "\n",
    "    if index_var.get() == 0:\n",
    "        data = d.TDFP\n",
    "    else:\n",
    "        data = d.DTFP\n",
    "\n",
    "    for terme in data:\n",
    "\n",
    "        if type(terme[0]) is str:\n",
    "            if text.lower() in terme[0].lower():\n",
    "                table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "        else:\n",
    "            if text.lower() in terme[1].lower():\n",
    "                table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "    # Add your search and processing logic here\n",
    "    pass\n",
    "\n",
    "def on_token_change(*args):\n",
    "    d.rebuild(Lancaster=stemming.get(),split=token.get())\n",
    "    search()\n",
    "\n",
    "def on_index_change(*args):\n",
    "    search()\n",
    "def show_match():\n",
    "    text = query_text.get()\n",
    "    table.delete(*table.get_children())\n",
    "\n",
    "    if selected_option.get() == \"SCALAR\":\n",
    "        rsv = d.MV(text,vect.SCALAR)\n",
    "    if selected_option.get() == \"COSINE\":\n",
    "        rsv = d.MV(text,vect.COSINE)\n",
    "    if selected_option.get() == \"JACCARD\":\n",
    "        rsv = d.MV(text,vect.JACCARD)\n",
    "\n",
    "    for doc in rsv.keys():\n",
    "        table.insert(\"\", \"end\", values=[doc,\"\",rsv[doc],\"\"])\n",
    "        \n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Processing Application\")\n",
    "\n",
    "# Search Query Text Box and Search Button\n",
    "query_label = tk.Label(root, text=\"Search Query:\")\n",
    "query_label.grid(row=0, column=0)\n",
    "query_text = tk.Entry(root, width=40)\n",
    "query_text.grid(row=0, column=1)\n",
    "search_button = tk.Button(root, text=\"Search\", command=search)\n",
    "search_button.grid(row=0, column=2)\n",
    "\n",
    "# Border around \"Processing\" section\n",
    "processing_label_frame = ttk.LabelFrame(root, text=\"Processing\")\n",
    "processing_label_frame.grid(row=1, column=2, padx=10, pady=10)\n",
    "\n",
    "# Radio buttons for Processing\n",
    "stemming = tk.IntVar()\n",
    "stemming.trace(\"w\", on_token_change)\n",
    "\n",
    "token = tk.IntVar()\n",
    "token.trace(\"w\", on_token_change)\n",
    "\n",
    "tokenization_radiobutton = tk.Checkbutton(processing_label_frame, text=\"Tokenization\", variable=token)\n",
    "porter_stemmer_radiobutton = tk.Checkbutton(processing_label_frame, text=\"Lancaster Stemmer\", variable=stemming)\n",
    "porter_stemmer_radiobutton.select()\n",
    "tokenization_radiobutton.grid(row=0, column=0)\n",
    "porter_stemmer_radiobutton.grid(row=1, column=0)\n",
    "\n",
    "# Border around \"Index\" section\n",
    "index_label_frame = ttk.LabelFrame(root, text=\"Index\")\n",
    "index_label_frame.grid(row=1, column=0, padx=10, pady=10)\n",
    "\n",
    "# Radio buttons for Index\n",
    "\n",
    "index_var = tk.IntVar()\n",
    "index_var.trace(\"w\", on_index_change)\n",
    "\n",
    "docs_per_term_radiobutton = tk.Radiobutton(index_label_frame, text=\"DOCS per TERM\", variable=index_var, value=0)\n",
    "terms_per_doc_radiobutton = tk.Radiobutton(index_label_frame, text=\"TERMS per DOC\", variable=index_var, value=1)\n",
    "docs_per_term_radiobutton.grid(row=1, column=0)\n",
    "terms_per_doc_radiobutton.grid(row=2, column=0)\n",
    "\n",
    "# Table to display data\n",
    "table_frame = ttk.Frame(root)\n",
    "table_frame.grid(row=2, column=0, columnspan=3, pady=10)\n",
    "table = ttk.Treeview(table_frame, columns=(\"1\", \"2\", \"3\", \"4\"))\n",
    "table.heading(\"#1\", text=\"terme\")\n",
    "table.heading(\"#2\", text=\"document\")\n",
    "table.heading(\"#3\", text=\"freq\")\n",
    "table.heading(\"#4\", text=\"Score\")\n",
    "table['show'] = 'headings'\n",
    "# Vertical scrollbar for the table\n",
    "table_scrollbar = ttk.Scrollbar(table_frame, orient=\"vertical\", command=table.yview)\n",
    "table.configure(yscrollcommand=table_scrollbar.set)\n",
    "\n",
    "# Grid layout for the table and scrollbar\n",
    "table.grid(row=0, column=0, sticky=\"nsew\")\n",
    "table_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n",
    "\n",
    "\n",
    "# Make the table resizable\n",
    "table_frame.grid_rowconfigure(0, weight=1)\n",
    "table_frame.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "# Border around \"Processing\" section\n",
    "Matching_label_frame = ttk.LabelFrame(root, text=\"Matching\")\n",
    "Matching_label_frame.grid(row=1, column=3, padx=10, pady=10)\n",
    "\n",
    "# Create a StringVar to hold the selected option\n",
    "selected_option = tk.StringVar()\n",
    "\n",
    "# Create a Combobox\n",
    "combo = ttk.Combobox(Matching_label_frame, textvariable=selected_option, state='readonly')\n",
    "combo['values'] = ('SCALAR','JACCARD', 'COSINE')  # Set the available choices\n",
    "combo.set('SCALAR')  # Set the default choice\n",
    "combo.grid(row=0, column=0)\n",
    "show_matching_button = tk.Button(Matching_label_frame, text=\"Show Matching\", command=show_match)\n",
    "show_matching_button.grid(row=1, column=0)\n",
    "\n",
    "for dat in d.TDFP:\n",
    "    table.insert(\"\", \"end\", values=[dat[0],dat[1],dat[2],dat[3]])\n",
    "\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
