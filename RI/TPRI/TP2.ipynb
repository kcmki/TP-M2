{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:100: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:100: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_12976\\2298217475.py:100: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0.6%', 4, 1, 0.16901960800285137),\n",
       " ('04', 5, 1, 0.07682709454675062),\n",
       " ('100', 2, 2, 0.2112745100035642),\n",
       " ('17', 2, 1, 0.1056372550017821),\n",
       " ('2019', 0, 1, 0.16901960800285137),\n",
       " ('2020', 0, 1, 0.16901960800285137),\n",
       " ('4,000', 2, 1, 0.1056372550017821),\n",
       " ('4.2%', 4, 1, 0.16901960800285137),\n",
       " ('500x', 2, 1, 0.1056372550017821),\n",
       " ('7b', 0, 1, 0.16901960800285137),\n",
       " ('abil', 2, 1, 0.1056372550017821),\n",
       " ('achiev', 0, 1, 0.16901960800285137),\n",
       " ('addit', 1, 1, 0.059640156839957804),\n",
       " ('addit', 2, 1, 0.059640156839957804),\n",
       " ('addit', 5, 1, 0.043374659519969314),\n",
       " ('address', 0, 1, 0.07958800173440753),\n",
       " ('address', 1, 1, 0.0497425010840047),\n",
       " ('address', 2, 1, 0.0497425010840047),\n",
       " ('address', 5, 1, 0.03617636442473069),\n",
       " ('adopt', 5, 1, 0.07682709454675062),\n",
       " ('advent', 1, 1, 0.1056372550017821),\n",
       " ('aim', 5, 1, 0.07682709454675062),\n",
       " ('allow', 3, 1, 0.06689555459199582),\n",
       " ('allow', 5, 1, 0.05473272648436022),\n",
       " ('also', 1, 1, 0.0497425010840047),\n",
       " ('also', 2, 1, 0.0497425010840047),\n",
       " ('also', 3, 1, 0.04421555651911529),\n",
       " ('also', 5, 1, 0.03617636442473069),\n",
       " ('although', 0, 1, 0.16901960800285137),\n",
       " ('among', 3, 1, 0.09389978222380631),\n",
       " ('amount', 3, 1, 0.09389978222380631),\n",
       " ('analysi', 1, 1, 0.1056372550017821),\n",
       " ('ann', 4, 5, 0.8450980400142568),\n",
       " ('anno-gpt', 2, 1, 0.1056372550017821),\n",
       " ('annot', 2, 6, 0.6338235300106926),\n",
       " ('ap', 5, 1, 0.07682709454675062),\n",
       " ('api', 0, 1, 0.16901960800285137),\n",
       " ('appli', 0, 1, 0.16901960800285137),\n",
       " ('approach', 0, 1, 0.09542425094393249),\n",
       " ('approach', 2, 1, 0.059640156839957804),\n",
       " ('approach', 3, 9, 0.47712125471966244),\n",
       " ('approxim', 4, 1, 0.16901960800285137),\n",
       " ('architectur', 1, 2, 0.2112745100035642),\n",
       " ('aspect-bas', 2, 1, 0.1056372550017821),\n",
       " ('assembl', 2, 1, 0.1056372550017821),\n",
       " ('assess', 1, 1, 0.1056372550017821),\n",
       " ('autoencod', 5, 1, 0.07682709454675062),\n",
       " ('b', 4, 1, 0.16901960800285137),\n",
       " ('base', 1, 1, 0.059640156839957804),\n",
       " ('base', 2, 1, 0.059640156839957804),\n",
       " ('base', 5, 1, 0.043374659519969314),\n",
       " ('baselin', 5, 1, 0.07682709454675062),\n",
       " ('begin', 3, 1, 0.09389978222380631),\n",
       " ('behind', 0, 2, 0.33803921600570275),\n",
       " ('beir', 3, 1, 0.09389978222380631),\n",
       " ('benchmark', 2, 1, 0.059640156839957804),\n",
       " ('benchmark', 3, 1, 0.05301347274662915),\n",
       " ('benchmark', 4, 2, 0.19084850188786498),\n",
       " ('bert', 1, 2, 0.2112745100035642),\n",
       " ('bert-bas', 1, 1, 0.1056372550017821),\n",
       " ('better', 2, 1, 0.1056372550017821),\n",
       " ('budget', 4, 1, 0.16901960800285137),\n",
       " ('build', 0, 1, 0.16901960800285137),\n",
       " ('built', 0, 1, 0.16901960800285137),\n",
       " ('capabl', 0, 1, 0.12041199826559248),\n",
       " ('capabl', 1, 1, 0.0752574989159953),\n",
       " ('captur', 4, 1, 0.16901960800285137),\n",
       " ('carri', 1, 1, 0.1056372550017821),\n",
       " ('case', 2, 2, 0.2112745100035642),\n",
       " ('chatgpt', 0, 1, 0.16901960800285137),\n",
       " ('clariti', 1, 1, 0.1056372550017821),\n",
       " ('clearli', 1, 1, 0.1056372550017821),\n",
       " ('collect', 2, 1, 0.0752574989159953),\n",
       " ('collect', 5, 1, 0.05473272648436022),\n",
       " ('combin', 1, 1, 0.1056372550017821),\n",
       " ('commonli', 5, 1, 0.07682709454675062),\n",
       " ('compar', 0, 1, 0.07958800173440753),\n",
       " ('compar', 1, 1, 0.0497425010840047),\n",
       " ('compar', 2, 1, 0.0497425010840047),\n",
       " ('compar', 3, 1, 0.04421555651911529),\n",
       " ('competit', 1, 1, 0.1056372550017821),\n",
       " ('complet', 1, 1, 0.1056372550017821),\n",
       " ('complex', 2, 7, 0.7394607850124747),\n",
       " ('compromis', 2, 1, 0.1056372550017821),\n",
       " ('comput', 2, 1, 0.0497425010840047),\n",
       " ('comput', 3, 2, 0.08843111303823058),\n",
       " ('comput', 4, 1, 0.07958800173440753),\n",
       " ('comput', 5, 1, 0.03617636442473069),\n",
       " ('condit', 5, 1, 0.07682709454675062),\n",
       " ('conduct', 1, 1, 0.1056372550017821),\n",
       " ('consid', 3, 1, 0.09389978222380631),\n",
       " ('consider', 3, 1, 0.09389978222380631),\n",
       " ('consist', 2, 1, 0.0752574989159953),\n",
       " ('consist', 3, 1, 0.06689555459199582),\n",
       " ('consumpt', 3, 2, 0.18779956444761262),\n",
       " ('context', 0, 1, 0.16901960800285137),\n",
       " ('contextu', 1, 2, 0.2112745100035642),\n",
       " ('contrast', 4, 1, 0.16901960800285137),\n",
       " ('convers', 3, 1, 0.09389978222380631),\n",
       " ('cost', 2, 2, 0.1505149978319906),\n",
       " ('cost', 3, 1, 0.06689555459199582),\n",
       " ('critic', 2, 1, 0.1056372550017821),\n",
       " ('cross-encod', 1, 1, 0.1056372550017821),\n",
       " ('data', 1, 1, 0.0752574989159953),\n",
       " ('data', 5, 1, 0.05473272648436022),\n",
       " ('dataset', 1, 1, 0.042802835102775785),\n",
       " ('dataset', 2, 6, 0.25681701061665474),\n",
       " ('dataset', 3, 1, 0.0380469645358007),\n",
       " ('dataset', 4, 1, 0.06848453616444126),\n",
       " ('dataset', 5, 1, 0.031129334620200573),\n",
       " ('date', 0, 1, 0.16901960800285137),\n",
       " ('de-facto', 4, 1, 0.16901960800285137),\n",
       " ('deep', 0, 1, 0.09542425094393249),\n",
       " ('deep', 1, 1, 0.059640156839957804),\n",
       " ('deep', 5, 1, 0.043374659519969314),\n",
       " ('deeper', 1, 2, 0.2112745100035642),\n",
       " ('demonstr', 1, 1, 0.0497425010840047),\n",
       " ('demonstr', 3, 2, 0.08843111303823058),\n",
       " ('demonstr', 4, 1, 0.07958800173440753),\n",
       " ('demonstr', 5, 1, 0.03617636442473069),\n",
       " ('dens', 1, 3, 0.22577249674798588),\n",
       " ('dens', 4, 2, 0.24082399653118497),\n",
       " ('design', 1, 1, 0.0752574989159953),\n",
       " ('design', 2, 1, 0.0752574989159953),\n",
       " ('dev', 4, 2, 0.33803921600570275),\n",
       " ('develop', 2, 1, 0.1056372550017821),\n",
       " ('discret', 4, 1, 0.16901960800285137),\n",
       " ('disjoint', 4, 1, 0.16901960800285137),\n",
       " ('dl', 3, 1, 0.09389978222380631),\n",
       " ('dl19', 4, 2, 0.33803921600570275),\n",
       " ('document', 1, 7, 0.2996198457194305),\n",
       " ('document', 2, 3, 0.12840850530832737),\n",
       " ('document', 3, 2, 0.0760939290716014),\n",
       " ('document', 4, 4, 0.27393814465776506),\n",
       " ('document', 5, 2, 0.062258669240401146),\n",
       " ('doris-ma', 2, 4, 0.4225490200071284),\n",
       " ('drop', 2, 1, 0.1056372550017821),\n",
       " ('dual', 4, 2, 0.33803921600570275),\n",
       " ('due', 2, 2, 0.2112745100035642),\n",
       " ('effect', 0, 2, 0.13696907232888253),\n",
       " ('effect', 1, 1, 0.042802835102775785),\n",
       " ('effect', 2, 2, 0.08560567020555157),\n",
       " ('effect', 3, 5, 0.1902348226790035),\n",
       " ('effect', 4, 1, 0.06848453616444126),\n",
       " ('effici', 3, 4, 0.2675822183679833),\n",
       " ('effici', 4, 2, 0.24082399653118497),\n",
       " ('effort', 2, 1, 0.1056372550017821),\n",
       " ('ehi', 4, 5, 0.8450980400142568),\n",
       " ('elasticsearch', 1, 1, 0.1056372550017821),\n",
       " ('emb', 4, 1, 0.16901960800285137),\n",
       " ('embed', 1, 1, 0.059640156839957804),\n",
       " ('embed', 4, 4, 0.38169700377572996),\n",
       " ('embed', 5, 2, 0.08674931903993863),\n",
       " ('embedding-bas', 1, 2, 0.1505149978319906),\n",
       " ('embedding-bas', 4, 1, 0.12041199826559248),\n",
       " ('emerg', 5, 1, 0.07682709454675062),\n",
       " ('empir', 3, 1, 0.09389978222380631),\n",
       " ('employ', 1, 1, 0.1056372550017821),\n",
       " ('encod', 1, 2, 0.1505149978319906),\n",
       " ('encod', 4, 2, 0.24082399653118497),\n",
       " ('end-to-end', 4, 1, 0.16901960800285137),\n",
       " ('endpoint', 0, 1, 0.16901960800285137),\n",
       " ('engin', 1, 1, 0.1056372550017821),\n",
       " ('enhanc', 3, 1, 0.09389978222380631),\n",
       " ('ensur', 4, 1, 0.16901960800285137),\n",
       " ('estim', 5, 1, 0.07682709454675062),\n",
       " ('evalu', 2, 2, 0.11928031367991561),\n",
       " ('evalu', 3, 2, 0.1060269454932583),\n",
       " ('evalu', 5, 1, 0.043374659519969314),\n",
       " ('exampl', 4, 1, 0.16901960800285137),\n",
       " ('exist', 2, 1, 0.059640156839957804),\n",
       " ('exist', 3, 1, 0.05301347274662915),\n",
       " ('exist', 5, 1, 0.043374659519969314),\n",
       " ('expans', 1, 2, 0.1505149978319906),\n",
       " ('expans', 5, 4, 0.21893090593744088),\n",
       " ('experi', 1, 1, 0.1056372550017821),\n",
       " ('experiment', 0, 2, 0.24082399653118497),\n",
       " ('experiment', 3, 1, 0.06689555459199582),\n",
       " ('expert', 2, 1, 0.1056372550017821),\n",
       " ('expert-level', 2, 1, 0.1056372550017821),\n",
       " ('extend', 2, 1, 0.1056372550017821),\n",
       " ('factor', 3, 1, 0.09389978222380631),\n",
       " ('feedback', 5, 2, 0.15365418909350123),\n",
       " ('field', 2, 1, 0.1056372550017821),\n",
       " ('file', 4, 1, 0.16901960800285137),\n",
       " ('final', 1, 2, 0.2112745100035642),\n",
       " ('find', 3, 1, 0.06689555459199582),\n",
       " ('find', 4, 1, 0.12041199826559248),\n",
       " ('fine-tun', 1, 1, 0.1056372550017821),\n",
       " ('first', 0, 1, 0.12041199826559248),\n",
       " ('first', 5, 1, 0.05473272648436022),\n",
       " ('first-of-its-kind', 3, 1, 0.09389978222380631),\n",
       " ('fix', 1, 1, 0.1056372550017821),\n",
       " ('focu', 5, 1, 0.07682709454675062),\n",
       " ('foundat', 0, 2, 0.33803921600570275),\n",
       " ('framework', 1, 1, 0.059640156839957804),\n",
       " ('framework', 2, 1, 0.059640156839957804),\n",
       " ('framework', 3, 1, 0.05301347274662915),\n",
       " ('full', 1, 1, 0.1056372550017821),\n",
       " ('fulli', 0, 1, 0.16901960800285137),\n",
       " ('furthermor', 2, 1, 0.1056372550017821),\n",
       " ('futur', 0, 1, 0.16901960800285137),\n",
       " ('gap', 1, 1, 0.0752574989159953),\n",
       " ('gap', 5, 1, 0.05473272648436022),\n",
       " ('gener', 1, 1, 0.0752574989159953),\n",
       " ('gener', 5, 2, 0.10946545296872044),\n",
       " ('given', 4, 2, 0.33803921600570275),\n",
       " ('gov02', 5, 1, 0.07682709454675062),\n",
       " ('gpt-3.5', 0, 1, 0.16901960800285137),\n",
       " ('gpt-4', 0, 1, 0.16901960800285137),\n",
       " ('handl', 2, 2, 0.2112745100035642),\n",
       " ('hidden', 0, 1, 0.16901960800285137),\n",
       " ('hierarch', 4, 1, 0.16901960800285137),\n",
       " ('high', 2, 1, 0.0752574989159953),\n",
       " ('high', 3, 3, 0.20068666377598746),\n",
       " ('high-qual', 0, 1, 0.16901960800285137),\n",
       " ('higher', 5, 1, 0.07682709454675062),\n",
       " ('highlight', 2, 1, 0.1056372550017821),\n",
       " ('hope', 0, 1, 0.16901960800285137),\n",
       " ('howev', 5, 1, 0.07682709454675062),\n",
       " ('human-author', 2, 1, 0.1056372550017821),\n",
       " ('identifi', 3, 1, 0.09389978222380631),\n",
       " ('ignor', 5, 1, 0.07682709454675062),\n",
       " ('ill-suit', 4, 1, 0.16901960800285137),\n",
       " ('impress', 3, 1, 0.09389978222380631),\n",
       " ('improv', 1, 1, 0.059640156839957804),\n",
       " ('improv', 3, 1, 0.05301347274662915),\n",
       " ('improv', 5, 1, 0.043374659519969314),\n",
       " ('includ', 4, 1, 0.16901960800285137),\n",
       " ('incur', 3, 1, 0.09389978222380631),\n",
       " ('index', 1, 1, 0.0752574989159953),\n",
       " ('index', 4, 2, 0.24082399653118497),\n",
       " ('indic', 3, 1, 0.09389978222380631),\n",
       " ('industri', 4, 2, 0.33803921600570275),\n",
       " ('infer', 3, 1, 0.09389978222380631),\n",
       " ('inform', 0, 1, 0.12041199826559248),\n",
       " ('inform', 5, 1, 0.05473272648436022),\n",
       " ('inher', 3, 1, 0.09389978222380631),\n",
       " ('input', 1, 1, 0.1056372550017821),\n",
       " ('interact', 1, 1, 0.0752574989159953),\n",
       " ('interact', 5, 2, 0.10946545296872044),\n",
       " ('introduc', 2, 1, 0.0752574989159953),\n",
       " ('introduc', 4, 1, 0.12041199826559248),\n",
       " ('invert', 4, 1, 0.16901960800285137),\n",
       " ('ir', 5, 2, 0.15365418909350123),\n",
       " ('ivf', 4, 1, 0.16901960800285137),\n",
       " ('jointli', 4, 1, 0.16901960800285137),\n",
       " ('labor', 2, 1, 0.1056372550017821),\n",
       " ('languag', 0, 1, 0.09542425094393249),\n",
       " ('languag', 2, 1, 0.059640156839957804),\n",
       " ('languag', 3, 1, 0.05301347274662915),\n",
       " ('larg', 0, 1, 0.07958800173440753),\n",
       " ('larg', 1, 1, 0.0497425010840047),\n",
       " ('larg', 2, 1, 0.0497425010840047),\n",
       " ('larg', 3, 1, 0.04421555651911529),\n",
       " ('latenc', 3, 1, 0.09389978222380631),\n",
       " ('latent', 5, 2, 0.15365418909350123),\n",
       " ('lead', 4, 1, 0.16901960800285137),\n",
       " ('learn', 0, 1, 0.07958800173440753),\n",
       " ('learn', 1, 2, 0.0994850021680094),\n",
       " ('learn', 4, 5, 0.3979400086720376),\n",
       " ('learn', 5, 2, 0.07235272884946138),\n",
       " ('length', 1, 1, 0.1056372550017821),\n",
       " ('lengthi', 1, 1, 0.1056372550017821),\n",
       " ('leverag', 1, 1, 0.0752574989159953),\n",
       " ('leverag', 5, 1, 0.05473272648436022),\n",
       " ('like', 3, 1, 0.06689555459199582),\n",
       " ('like', 4, 1, 0.12041199826559248),\n",
       " ('limit', 1, 1, 0.0752574989159953),\n",
       " ('limit', 2, 1, 0.0752574989159953),\n",
       " ('listwis', 0, 1, 0.12041199826559248),\n",
       " ('listwis', 3, 1, 0.06689555459199582),\n",
       " ('llm', 0, 3, 0.28627275283179743),\n",
       " ('llm', 2, 2, 0.11928031367991561),\n",
       " ('llm', 3, 2, 0.1060269454932583),\n",
       " ('llm-base', 3, 3, 0.28169934667141894),\n",
       " ('low-dimension', 1, 1, 0.1056372550017821),\n",
       " ('mani', 5, 1, 0.07682709454675062),\n",
       " ('map', 5, 1, 0.07682709454675062),\n",
       " ('marco', 4, 2, 0.24082399653118497),\n",
       " ('marco', 5, 1, 0.05473272648436022),\n",
       " ('match', 1, 2, 0.1505149978319906),\n",
       " ('match', 5, 1, 0.05473272648436022),\n",
       " ('maximum', 1, 1, 0.1056372550017821),\n",
       " ('method', 2, 1, 0.059640156839957804),\n",
       " ('method', 3, 1, 0.05301347274662915),\n",
       " ('method', 4, 1, 0.09542425094393249),\n",
       " ('might', 4, 1, 0.16901960800285137),\n",
       " ('model', 0, 3, 0.1806179973983887),\n",
       " ('model', 1, 7, 0.26340124620598354),\n",
       " ('model', 2, 1, 0.03762874945799765),\n",
       " ('model', 3, 2, 0.06689555459199582),\n",
       " ('model', 4, 1, 0.06020599913279624),\n",
       " ('model', 5, 11, 0.3010299956639812),\n",
       " ('modern', 0, 1, 0.16901960800285137),\n",
       " ('mostli', 0, 1, 0.16901960800285137),\n",
       " ('mrr@10', 4, 1, 0.16901960800285137),\n",
       " ('ms', 4, 2, 0.24082399653118497),\n",
       " ('ms', 5, 1, 0.05473272648436022),\n",
       " ('much', 0, 1, 0.16901960800285137),\n",
       " ('multi-level', 2, 1, 0.1056372550017821),\n",
       " ('multi-ti', 2, 1, 0.1056372550017821),\n",
       " ('multifacet', 2, 2, 0.2112745100035642),\n",
       " ('natur', 2, 1, 0.1056372550017821),\n",
       " ('ndcg@10', 4, 1, 0.16901960800285137),\n",
       " ('nearest', 4, 1, 0.16901960800285137),\n",
       " ('need', 2, 1, 0.1056372550017821),\n",
       " ('neighbor', 4, 1, 0.16901960800285137),\n",
       " ('neural', 5, 3, 0.23048128364025183),\n",
       " ('non-determinist', 0, 1, 0.16901960800285137),\n",
       " ('notabl', 2, 1, 0.1056372550017821),\n",
       " ('notion', 4, 1, 0.16901960800285137),\n",
       " ('novel', 2, 1, 0.0752574989159953),\n",
       " ('novel', 3, 1, 0.06689555459199582),\n",
       " ('number', 3, 1, 0.09389978222380631),\n",
       " ('observ', 2, 1, 0.1056372550017821),\n",
       " ('obtain', 4, 1, 0.16901960800285137),\n",
       " ('often', 5, 1, 0.07682709454675062),\n",
       " ('ohsum', 1, 1, 0.1056372550017821),\n",
       " ('opaqu', 0, 1, 0.16901960800285137),\n",
       " ('open-sourc', 0, 1, 0.16901960800285137),\n",
       " ('optim', 4, 1, 0.16901960800285137),\n",
       " ('other', 3, 1, 0.09389978222380631),\n",
       " ('outcom', 0, 1, 0.16901960800285137),\n",
       " ('outperform', 4, 1, 0.12041199826559248),\n",
       " ('outperform', 5, 1, 0.05473272648436022),\n",
       " ('overhead', 3, 1, 0.09389978222380631),\n",
       " ('pair', 5, 1, 0.07682709454675062),\n",
       " ('pairwis', 3, 2, 0.18779956444761262),\n",
       " ('paramet', 0, 1, 0.16901960800285137),\n",
       " ('passag', 5, 1, 0.07682709454675062),\n",
       " ('path', 4, 1, 0.16901960800285137),\n",
       " ('perform', 0, 1, 0.06848453616444126),\n",
       " ('perform', 1, 2, 0.08560567020555157),\n",
       " ('perform', 2, 2, 0.08560567020555157),\n",
       " ('perform', 4, 2, 0.13696907232888253),\n",
       " ('perform', 5, 1, 0.031129334620200573),\n",
       " ('phrase', 1, 2, 0.2112745100035642),\n",
       " ('pipelin', 1, 1, 0.1056372550017821),\n",
       " ('pointwis', 3, 2, 0.18779956444761262),\n",
       " ('poor', 3, 1, 0.09389978222380631),\n",
       " ('posit', 4, 1, 0.16901960800285137),\n",
       " ('present', 0, 1, 0.16901960800285137),\n",
       " ('primarili', 2, 1, 0.1056372550017821),\n",
       " ('problem', 4, 1, 0.16901960800285137),\n",
       " ('procedur', 3, 1, 0.09389978222380631),\n",
       " ('process', 4, 1, 0.16901960800285137),\n",
       " ('produc', 1, 2, 0.1505149978319906),\n",
       " ('produc', 2, 1, 0.0752574989159953),\n",
       " ('prompt', 3, 3, 0.28169934667141894),\n",
       " ('propos', 1, 1, 0.042802835102775785),\n",
       " ('propos', 2, 1, 0.042802835102775785),\n",
       " ('propos', 3, 2, 0.0760939290716014),\n",
       " ('propos', 4, 1, 0.06848453616444126),\n",
       " ('propos', 5, 3, 0.09338800386060171),\n",
       " ('proprietari', 0, 1, 0.16901960800285137),\n",
       " ('provid', 0, 1, 0.16901960800285137),\n",
       " ('pseudo-relev', 5, 1, 0.07682709454675062),\n",
       " ('qe', 5, 4, 0.30730837818700246),\n",
       " ('qualiti', 2, 1, 0.1056372550017821),\n",
       " ('queri', 1, 8, 0.3979400086720376),\n",
       " ('queri', 2, 8, 0.3979400086720376),\n",
       " ('queri', 4, 4, 0.3183520069376301),\n",
       " ('queri', 5, 2, 0.07235272884946138),\n",
       " ('query-docu', 5, 3, 0.23048128364025183),\n",
       " ('query-specif', 1, 1, 0.1056372550017821),\n",
       " ('query/docu', 4, 1, 0.16901960800285137),\n",
       " ('rank', 2, 1, 0.0497425010840047),\n",
       " ('rank', 3, 7, 0.309508895633807),\n",
       " ('rank', 4, 1, 0.07958800173440753),\n",
       " ('rank', 5, 2, 0.07235272884946138),\n",
       " ('rankvicuna', 0, 1, 0.16901960800285137),\n",
       " ('re-rank', 1, 1, 0.1056372550017821),\n",
       " ('recent', 2, 1, 0.0752574989159953),\n",
       " ('recent', 5, 1, 0.05473272648436022),\n",
       " ('recogn', 2, 1, 0.1056372550017821),\n",
       " ('record', 1, 1, 0.1056372550017821),\n",
       " ('reduc', 1, 1, 0.0752574989159953),\n",
       " ('reduc', 3, 2, 0.13379110918399165),\n",
       " ('reduct', 2, 1, 0.1056372550017821),\n",
       " ('relev', 1, 1, 0.0497425010840047),\n",
       " ('relev', 2, 3, 0.14922750325201412),\n",
       " ('relev', 4, 1, 0.07958800173440753),\n",
       " ('relev', 5, 2, 0.07235272884946138),\n",
       " ('remain', 0, 1, 0.16901960800285137),\n",
       " ('repres', 1, 1, 0.0752574989159953),\n",
       " ('repres', 2, 1, 0.0752574989159953),\n",
       " ('represent', 1, 3, 0.3169117650053463),\n",
       " ('reproduc', 0, 1, 0.16901960800285137),\n",
       " ('requir', 2, 2, 0.2112745100035642),\n",
       " ('rerank', 0, 5, 0.8450980400142568),\n",
       " ('research', 0, 2, 0.19084850188786498),\n",
       " ('research', 2, 3, 0.17892047051987342),\n",
       " ('research', 5, 2, 0.08674931903993863),\n",
       " ('resourc', 2, 1, 0.1056372550017821),\n",
       " ('result', 0, 2, 0.13696907232888253),\n",
       " ('result', 1, 3, 0.12840850530832737),\n",
       " ('result', 2, 1, 0.042802835102775785),\n",
       " ('result', 3, 1, 0.0380469645358007),\n",
       " ('result', 5, 1, 0.031129334620200573),\n",
       " ('retain', 3, 1, 0.09389978222380631),\n",
       " ('retriev', 0, 1, 0.06848453616444126),\n",
       " ('retriev', 1, 3, 0.12840850530832737),\n",
       " ('retriev', 2, 3, 0.12840850530832737),\n",
       " ('retriev', 4, 2, 0.13696907232888253),\n",
       " ('retriev', 5, 1, 0.031129334620200573),\n",
       " ('richer', 1, 1, 0.1056372550017821),\n",
       " ('robust', 5, 1, 0.07682709454675062),\n",
       " ('sbert', 1, 1, 0.1056372550017821),\n",
       " ('scalabl', 2, 1, 0.1056372550017821),\n",
       " ('scienc', 2, 1, 0.1056372550017821),\n",
       " ('scientif', 2, 4, 0.4225490200071284),\n",
       " ('score', 2, 1, 0.0752574989159953),\n",
       " ('score', 3, 1, 0.06689555459199582),\n",
       " ('search', 1, 1, 0.0752574989159953),\n",
       " ('search', 4, 2, 0.24082399653118497),\n",
       " ('self-attent', 1, 1, 0.1056372550017821),\n",
       " ('semant', 1, 2, 0.11928031367991561),\n",
       " ('semant', 4, 1, 0.09542425094393249),\n",
       " ('semant', 5, 1, 0.043374659519969314),\n",
       " ('sentenc', 1, 1, 0.1056372550017821),\n",
       " ('separ', 1, 2, 0.2112745100035642),\n",
       " ('set', 0, 1, 0.12041199826559248),\n",
       " ('set', 4, 2, 0.24082399653118497),\n",
       " ('setwis', 3, 1, 0.09389978222380631),\n",
       " ('sever', 4, 1, 0.16901960800285137),\n",
       " ('shaki', 0, 1, 0.16901960800285137),\n",
       " ('short', 1, 1, 0.1056372550017821),\n",
       " ('shortcom', 0, 1, 0.16901960800285137),\n",
       " ('show', 0, 1, 0.12041199826559248),\n",
       " ('show', 5, 1, 0.05473272648436022),\n",
       " ('signific', 0, 1, 0.12041199826559248),\n",
       " ('signific', 2, 1, 0.0752574989159953),\n",
       " ('significantli', 3, 1, 0.09389978222380631),\n",
       " ('similar', 4, 1, 0.12041199826559248),\n",
       " ('similar', 5, 1, 0.05473272648436022),\n",
       " ('size', 1, 1, 0.0752574989159953),\n",
       " ('size', 3, 1, 0.06689555459199582),\n",
       " ('slightli', 0, 1, 0.16901960800285137),\n",
       " ('smaller', 0, 1, 0.16901960800285137),\n",
       " ('sota', 4, 1, 0.16901960800285137),\n",
       " ('space', 1, 2, 0.1505149978319906),\n",
       " ('space', 5, 2, 0.10946545296872044),\n",
       " ('stabl', 4, 1, 0.16901960800285137),\n",
       " ('stage', 4, 1, 0.16901960800285137),\n",
       " ('standard', 4, 3, 0.3612359947967774),\n",
       " ('standard', 5, 1, 0.05473272648436022),\n",
       " ('state-of-the-art', 4, 1, 0.12041199826559248),\n",
       " ('state-of-the-art', 5, 1, 0.05473272648436022),\n",
       " ('structur', 2, 1, 0.0752574989159953),\n",
       " ('structur', 4, 3, 0.3612359947967774),\n",
       " ('studi', 1, 2, 0.1505149978319906),\n",
       " ('studi', 3, 1, 0.06689555459199582),\n",
       " ('style', 4, 1, 0.16901960800285137),\n",
       " ('sub-queri', 2, 1, 0.1056372550017821),\n",
       " ('suboptim', 4, 1, 0.16901960800285137),\n",
       " ('success', 0, 1, 0.16901960800285137),\n",
       " ('suffer', 3, 1, 0.09389978222380631),\n",
       " ('summar', 1, 1, 0.1056372550017821),\n",
       " ('superior', 3, 1, 0.09389978222380631),\n",
       " ('task', 2, 3, 0.22577249674798588),\n",
       " ('task', 3, 1, 0.06689555459199582),\n",
       " ('techniqu', 4, 1, 0.16901960800285137),\n",
       " ('term', 5, 1, 0.07682709454675062),\n",
       " ('test', 2, 1, 0.059640156839957804),\n",
       " ('test', 3, 1, 0.05301347274662915),\n",
       " ('test', 5, 1, 0.043374659519969314),\n",
       " ('text', 1, 4, 0.4225490200071284),\n",
       " ('thoroughli', 3, 1, 0.09389978222380631),\n",
       " ('threaten', 0, 1, 0.16901960800285137),\n",
       " ('three', 5, 1, 0.07682709454675062),\n",
       " ('time', 5, 1, 0.07682709454675062),\n",
       " ('token', 1, 1, 0.0752574989159953),\n",
       " ('token', 3, 2, 0.13379110918399165),\n",
       " ('track', 0, 1, 0.16901960800285137),\n",
       " ('trade-off', 3, 1, 0.09389978222380631),\n",
       " ('tradit', 2, 1, 0.0752574989159953),\n",
       " ('tradit', 5, 2, 0.10946545296872044),\n",
       " ('train', 4, 1, 0.12041199826559248),\n",
       " ('train', 5, 1, 0.05473272648436022),\n",
       " ('transformer-bas', 1, 3, 0.3169117650053463),\n",
       " ('trec', 0, 1, 0.07958800173440753),\n",
       " ('trec', 3, 1, 0.04421555651911529),\n",
       " ('trec', 4, 2, 0.15917600346881505),\n",
       " ('trec', 5, 1, 0.03617636442473069),\n",
       " ('trec-cds-2014', 1, 1, 0.1056372550017821),\n",
       " ('tree', 4, 2, 0.33803921600570275),\n",
       " ('tree-bas', 4, 1, 0.16901960800285137),\n",
       " ('two', 1, 1, 0.0752574989159953),\n",
       " ('two', 4, 1, 0.12041199826559248),\n",
       " ('two-stag', 4, 1, 0.16901960800285137),\n",
       " ('understand', 1, 2, 0.2112745100035642),\n",
       " ('us', 3, 1, 0.09389978222380631),\n",
       " ('use', 1, 2, 0.08560567020555157),\n",
       " ('use', 2, 1, 0.042802835102775785),\n",
       " ('use', 3, 1, 0.0380469645358007),\n",
       " ('use', 4, 2, 0.13696907232888253),\n",
       " ('use', 5, 3, 0.09338800386060171),\n",
       " ('user', 2, 1, 0.1056372550017821),\n",
       " ('valid', 2, 1, 0.1056372550017821),\n",
       " ('variat', 5, 1, 0.07682709454675062),\n",
       " ('vector', 1, 5, 0.5281862750089105),\n",
       " ('verac', 0, 1, 0.16901960800285137),\n",
       " ('vice-versa', 4, 1, 0.16901960800285137),\n",
       " ('web', 4, 1, 0.16901960800285137),\n",
       " ('well-known', 1, 1, 0.1056372550017821),\n",
       " ('within', 2, 1, 0.0752574989159953),\n",
       " ('within', 3, 1, 0.06689555459199582),\n",
       " ('without', 2, 2, 0.2112745100035642),\n",
       " ('work', 0, 2, 0.24082399653118497),\n",
       " ('work', 4, 1, 0.12041199826559248),\n",
       " ('year', 5, 1, 0.07682709454675062),\n",
       " ('yield', 0, 1, 0.12041199826559248),\n",
       " ('yield', 1, 1, 0.0752574989159953),\n",
       " ('zero-shot', 0, 2, 0.24082399653118497),\n",
       " ('zero-shot', 3, 6, 0.4013733275519749)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class vect:\n",
    "    SCALAR = 1\n",
    "    COSINE = 2\n",
    "    JACCARD = 3\n",
    "\n",
    "class Docs:\n",
    "\n",
    "    def __init__(self,Lancaster=True,split=False):\n",
    "\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "\n",
    "        files = os.listdir(\"./TP2data/\")\n",
    "        self.txt_files = [file for file in files if file.endswith(\".txt\")]\n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.txt_files)):\n",
    "            \n",
    "            file = open(\"./TP2data/\"+self.txt_files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "            Termes = self.splitter(file)\n",
    "\n",
    "            TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "            TermesNormalisation = [self.normalize(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term,i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "    \n",
    "    def rebuild(self,Lancaster=True,split=False):\n",
    "        docs = []\n",
    "        self.maxfreq = {}\n",
    "        MotsVides = nltk.corpus.stopwords.words('english')\n",
    "        self.Lancaster = Lancaster\n",
    "        self.split = split\n",
    "        \n",
    "        file = \"\"\n",
    "        my_dict = {}\n",
    "        for i in range(len(self.txt_files)):\n",
    "            file = open(\"./TP2data/\"+self.txt_files[i]).read()\n",
    "            file = file.replace(\"\\n\",\"\")\n",
    "\n",
    "            Termes = self.splitter(file)\n",
    "\n",
    "            TermesSansMotsVides = [terme for terme in Termes if terme.lower() not in MotsVides]\n",
    "            TermesNormalisation = [self.normalize(terme) for terme in TermesSansMotsVides]\n",
    "\n",
    "            for term in TermesNormalisation:\n",
    "                \n",
    "                if (term.lower(),i) in my_dict:\n",
    "                    my_dict[(term,i)] += 1\n",
    "                else:\n",
    "                    my_dict[(term,i)] = 1\n",
    "\n",
    "        self.dict = my_dict\n",
    "\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[1] not in docs:\n",
    "                docs.append(keys[1])\n",
    "            if keys[1] not in self.maxfreq.keys():\n",
    "                self.maxfreq[keys[1]] = 0\n",
    "            if self.dict[keys] > self.maxfreq[keys[1]]:\n",
    "\n",
    "                self.maxfreq[keys[1]] = self.dict[keys]\n",
    "\n",
    "        self.N = len(docs)\n",
    "        self.DictWeight()\n",
    "    def normalize(self,text):\n",
    "        if self.Lancaster :\n",
    "            Lancaster = nltk.LancasterStemmer()\n",
    "            text = Lancaster.stem(text)\n",
    "        else :\n",
    "            porter = nltk.PorterStemmer()\n",
    "            text = porter.stem(text)\n",
    "        return text\n",
    "    def splitter(self,text):\n",
    "        if not self.split:\n",
    "            return text.split(\" \")\n",
    "        else:\n",
    "            ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] >>> \n",
    "            Termes = ExpReg.tokenize(text) \n",
    "            return Termes\n",
    "    def calcWeight(self,word,doc):\n",
    "\n",
    "        inDocs = []\n",
    "\n",
    "        for keys in self.dict.keys():\n",
    "            if keys[0] == word:\n",
    "                if keys[1] not in inDocs:\n",
    "                    inDocs.append(keys[1])\n",
    "\n",
    "        # nombre de documents contenant le mot\n",
    "        Ni = len(inDocs)\n",
    "\n",
    "        try:\n",
    "            tf = self.dict[(word,doc)]/self.maxfreq[doc]\n",
    "        except:\n",
    "            tf = 0\n",
    "            \n",
    "        #idf = math.log10((N/Ni)+1)\n",
    "        try:\n",
    "            idf = math.log10((self.N/Ni)+1)\n",
    "        except:\n",
    "            idf = 0\n",
    "\n",
    "        return(tf*idf)\n",
    "    def DictWeight(self):\n",
    "\n",
    "        self.TDFP = [(word,doc,self.dict[(word,doc)],self.calcWeight(word,doc)) for word,doc in self.dict.keys()]\n",
    "        self.DTFP = [(doc,word,freq,weight) for word,doc,freq,weight in self.TDFP]\n",
    "        self.TDFP = sorted(self.TDFP, key=lambda x: x[0])\n",
    "    def MV(self,termes,typeV = vect.SCALAR):\n",
    "        rsv = {}\n",
    "        termes = self.splitter(termes)\n",
    "        normterms = []\n",
    "        for term in termes:\n",
    "            normterms.append(self.normalize(term))\n",
    "\n",
    "        for doc in range(self.N):\n",
    "            QD = 0\n",
    "            v2 = 0\n",
    "            w2 = 0\n",
    "\n",
    "            for term in normterms:\n",
    "                if (term.lower(),doc) in self.dict.keys():\n",
    "                    weight = self.calcWeight(term,doc)\n",
    "                    QD+= weight\n",
    "                    w2+= weight**2\n",
    "                    v2+= 1**2\n",
    "            try:\n",
    "                if typeV == vect.COSINE:\n",
    "                    QD = QD / (math.sqrt(v2)*math.sqrt(w2))\n",
    "                if typeV == vect.JACCARD:\n",
    "                    QD = QD / (w2+v2 - QD)\n",
    "            except ZeroDivisionError:\n",
    "                QD = 0\n",
    "            rsv[doc] = QD\n",
    "\n",
    "        rsv = dict(sorted(rsv.items(), key=lambda item: item[1],reverse=True))\n",
    "        return rsv\n",
    "\n",
    "    def bm25(self,termes,K,B):\n",
    "        rsv = {}\n",
    "        #tokenization de la requete\n",
    "        termes = self.splitter(termes)\n",
    "        normterms = []\n",
    "        dls = {}\n",
    "        #normalisation de la requete \n",
    "        for term in termes:\n",
    "            normterms.append(self.normalize(term.lower()))\n",
    "\n",
    "        # calcule nombre termes par doc\n",
    "        for term,doc in self.dict.keys():\n",
    "            if doc in dls.keys():\n",
    "                dls[doc] +=1\n",
    "            else:\n",
    "                dls[doc] = 1\n",
    "\n",
    "        # moyenne des termes par doc\n",
    "        avdl = 0\n",
    "        for key in dls.keys():\n",
    "            avdl += dls[key]\n",
    "        avdl = avdl / len(dls.keys())\n",
    "\n",
    "        for doc in range(self.N):\n",
    "            QD = 0\n",
    "            for term in normterms:\n",
    "                val = 0\n",
    "                log = 0\n",
    "                if (term,doc) in self.dict.keys():\n",
    "                    ni = 0\n",
    "                    \n",
    "                    for doc2 in range(self.N):\n",
    "                        if (term.lower(),doc2) in self.dict.keys():\n",
    "                            ni+=1\n",
    "                    freq = self.dict[(term.lower(),doc)]\n",
    "                    log = math.log10((self.N - ni + 0.5) / (ni + 0.5))\n",
    "                    val = freq + K*((1-B)+(B*dls[doc]/avdl))\n",
    "                    val = freq/val\n",
    "                QD+= val*log\n",
    "            rsv[doc] = QD\n",
    "            \n",
    "        #trie du dictionnaire\n",
    "        rsv = dict(sorted(rsv.items(), key=lambda item: item[1],reverse=True))\n",
    "        return rsv\n",
    "    def boolean(self,text):\n",
    "        text = self.splitter(text)\n",
    "        docText = {}\n",
    "        for doc in range(d.N):\n",
    "            copy = text.copy()\n",
    "            lexique = [\" * \",\" - \",\" + \",\" 1 \"]\n",
    "            copy = [\" * \" if item == \"AND\" else item for item in copy]\n",
    "            copy = [\" - \" if item == \"NOT\" else item for item in copy]\n",
    "            copy = [\" + \" if item == \"OR\" else item for item in copy]\n",
    "            copy = [\" 1 \" if (self.normalize(item),doc) in self.dict.keys() else item for item in copy]\n",
    "            copy = [\" 0 \" if item not in lexique  else item for item in copy]\n",
    "            new_copy = []\n",
    "            i=0\n",
    "            while(i<len(copy)):\n",
    "                if copy[i] == \" - \" and copy[i+1] == \" 0 \":\n",
    "                    new_copy.append(\" 1 \")\n",
    "                    i+=2\n",
    "                elif copy[i] == \" - \" and copy[i+1] == \" 1 \":\n",
    "                    new_copy.append(\" 0 \")\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_copy.append(copy[i])\n",
    "                    i+=1\n",
    "            docText[doc] = 1 if eval(\"\".join(new_copy)) > 0 else 0\n",
    "        return docText\n",
    "d = Docs(Lancaster=False,split=True)\n",
    "d.DictWeight()\n",
    "\n",
    "d.TDFP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_12976\\2208073675.py\", line 40, in on_token_change\n",
      "    search()\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_12976\\2208073675.py\", line 8, in search\n",
      "    table.delete(*table.get_children())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\ttk.py\", line 1195, in get_children\n",
      "    self.tk.call(self._w, \"children\", item or '') or ())\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "_tkinter.TclError: invalid command name \".!frame.!treeview\"\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_12976\\2208073675.py\", line 40, in on_token_change\n",
      "    search()\n",
      "  File \"C:\\Users\\Mekki\\AppData\\Local\\Temp\\ipykernel_12976\\2208073675.py\", line 8, in search\n",
      "    table.delete(*table.get_children())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\APPS\\python\\Lib\\tkinter\\ttk.py\", line 1195, in get_children\n",
      "    self.tk.call(self._w, \"children\", item or '') or ())\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "_tkinter.TclError: invalid command name \".!frame.!treeview\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n",
      "empty\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "def search():\n",
    "    global stemming\n",
    "\n",
    "    text = query_text.get()\n",
    "    table.delete(*table.get_children())\n",
    "    if index_var.get() == 0:\n",
    "        data = d.TDFP\n",
    "    else:\n",
    "        data = d.DTFP\n",
    "\n",
    "    if text == \"\":\n",
    "        print(\"empty\")\n",
    "        for terme in data:\n",
    "            if type(terme[0]) is str:\n",
    "                if text.lower() in terme[0].lower():\n",
    "                    table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "            else:\n",
    "                if text.lower() in terme[1].lower():\n",
    "                    table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "        return\n",
    "    \n",
    "    searchText = d.splitter(text)\n",
    "    searchText = [d.normalize(terme) for terme in searchText]\n",
    "\n",
    "    for text in searchText:\n",
    "        for terme in data:\n",
    "            if type(terme[0]) is str:\n",
    "                if text.lower() in terme[0].lower():\n",
    "                    table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "            else:\n",
    "                if text.lower() in terme[1].lower():\n",
    "                    table.insert(\"\", \"end\", values=[terme[0],terme[1],terme[2],terme[3]])\n",
    "\n",
    "def on_token_change(*args):\n",
    "    d.rebuild(Lancaster=stemming.get(),split=token.get())\n",
    "    search()\n",
    "\n",
    "def on_index_change(*args):\n",
    "    search()\n",
    "def show_match():\n",
    "    text = query_text.get()\n",
    "    table.delete(*table.get_children())\n",
    "\n",
    "    if selected_option.get() == \"SCALAR\":\n",
    "        rsv = d.MV(text,vect.SCALAR)\n",
    "    if selected_option.get() == \"COSINE\":\n",
    "        rsv = d.MV(text,vect.COSINE)\n",
    "    if selected_option.get() == \"JACCARD\":\n",
    "        rsv = d.MV(text,vect.JACCARD)\n",
    "\n",
    "    for doc in rsv.keys():\n",
    "        table.insert(\"\", \"end\", values=[doc,\"\",rsv[doc],\"\"])\n",
    "def show_bm25():\n",
    "    text = query_text.get()\n",
    "    table.delete(*table.get_children())\n",
    "    b = float(input_B.get())\n",
    "    k = float(input_K.get())\n",
    "    rsv = d.bm25(text,k,b)\n",
    "    for doc in rsv.keys():\n",
    "        table.insert(\"\", \"end\", values=[doc,\"\",rsv[doc],\"\"])\n",
    "def show_boolean_model():\n",
    "    global stemming\n",
    "    table.delete(*table.get_children())\n",
    "    text = query_text.get()\n",
    "    docText = d.boolean(text)\n",
    "    for doc in docText.keys():\n",
    "        table.insert(\"\", \"end\", values=[doc,\"\",docText[doc],\"\"])\n",
    "def show_datamining_model():\n",
    "    \n",
    "    pass\n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Processing Application\")\n",
    "\n",
    "# Search Query Text Box and Search Button\n",
    "search_label_frame = ttk.LabelFrame(root, text=\"Search\")\n",
    "search_label_frame.grid(row=0, column=0,columnspan=2, padx=10, pady=10)\n",
    "query_label = tk.Label(search_label_frame, text=\"Search Query:\")\n",
    "query_label.grid(row=0, column=0)\n",
    "query_text = tk.Entry(search_label_frame, width=40)\n",
    "query_text.grid(row=0, column=1)\n",
    "search_button = tk.Button(search_label_frame, text=\"Search\", command=search)\n",
    "search_button.grid(row=0, column=2)\n",
    "\n",
    "# Border around \"Processing\" section\n",
    "processing_label_frame = ttk.LabelFrame(root, text=\"Processing\")\n",
    "processing_label_frame.grid(row=1, column=1, padx=10, pady=10)\n",
    "\n",
    "# Radio buttons for Processing\n",
    "stemming = tk.IntVar()\n",
    "stemming.trace(\"w\", on_token_change)\n",
    "\n",
    "token = tk.IntVar()\n",
    "token.trace(\"w\", on_token_change)\n",
    "\n",
    "tokenization_radiobutton = tk.Checkbutton(processing_label_frame, text=\"Tokenization\", variable=token)\n",
    "porter_stemmer_radiobutton = tk.Checkbutton(processing_label_frame, text=\"Lancaster Stemmer\", variable=stemming)\n",
    "porter_stemmer_radiobutton.select()\n",
    "tokenization_radiobutton.select()\n",
    "tokenization_radiobutton.grid(row=0, column=0)\n",
    "porter_stemmer_radiobutton.grid(row=1, column=0)\n",
    "\n",
    "# Border around \"Index\" section\n",
    "index_label_frame = ttk.LabelFrame(root, text=\"Index\")\n",
    "index_label_frame.grid(row=1, column=0, padx=10, pady=10)\n",
    "index_var = tk.IntVar()\n",
    "index_var.trace(\"w\", on_index_change)\n",
    "docs_per_term_radiobutton = tk.Radiobutton(index_label_frame, text=\"DOCS per TERM\", variable=index_var, value=0)\n",
    "terms_per_doc_radiobutton = tk.Radiobutton(index_label_frame, text=\"TERMS per DOC\", variable=index_var, value=1)\n",
    "docs_per_term_radiobutton.grid(row=1, column=0)\n",
    "terms_per_doc_radiobutton.grid(row=2, column=0)\n",
    "\n",
    "# Table to display data\n",
    "table_frame = ttk.Frame(root)\n",
    "table_frame.grid(row=2, column=0, columnspan=2,rowspan=2, pady=10)\n",
    "table = ttk.Treeview(table_frame, columns=(\"1\", \"2\", \"3\", \"4\"))\n",
    "table.heading(\"#1\", text=\"terme\")\n",
    "table.heading(\"#2\", text=\"document\")\n",
    "table.heading(\"#3\", text=\"freq\")\n",
    "table.heading(\"#4\", text=\"Score\")\n",
    "table['show'] = 'headings'\n",
    "# Vertical scrollbar for the table\n",
    "table_scrollbar = ttk.Scrollbar(table_frame, orient=\"vertical\", command=table.yview)\n",
    "table.configure(yscrollcommand=table_scrollbar.set)\n",
    "\n",
    "# Grid layout for the table and scrollbar\n",
    "table.grid(row=0, column=0, sticky=\"nsew\")\n",
    "table_scrollbar.grid(row=0, column=1, sticky=\"ns\")\n",
    "\n",
    "\n",
    "# Make the table resizable\n",
    "table_frame.grid_rowconfigure(0, weight=1)\n",
    "table_frame.grid_columnconfigure(0, weight=1)\n",
    "\n",
    "# ----------------------------------\n",
    "# Border around \"Processing\" section\n",
    "# ----------------------------------\n",
    "Matching_label_frame = ttk.LabelFrame(root, text=\"Matching\")\n",
    "Matching_label_frame.grid(row=1, column=2,rowspan=2, padx=10, pady=10)\n",
    "# Vector space model\n",
    "VectorSpaceModel_label_frame = ttk.LabelFrame(Matching_label_frame, text=\"Vector space model\")\n",
    "VectorSpaceModel_label_frame.grid(row=0, column=0, padx=10, pady=10)\n",
    "selected_option = tk.StringVar()\n",
    "combo = ttk.Combobox(VectorSpaceModel_label_frame, textvariable=selected_option, state='readonly')\n",
    "combo['values'] = ('SCALAR','JACCARD', 'COSINE')  \n",
    "combo.set('SCALAR') \n",
    "combo.grid(row=0, column=0)\n",
    "show_matching_button = tk.Button(VectorSpaceModel_label_frame, text=\"Vector space model\", command=show_match)\n",
    "show_matching_button.grid(row=1, column=0)\n",
    "# BM 25\n",
    "bm25_label_frame = ttk.LabelFrame(Matching_label_frame, text=\"Bm25\")\n",
    "bm25_label_frame.grid(row=0, column=1, padx=10, pady=10)\n",
    "tk.Label(bm25_label_frame, text=\"K\").grid(row=0, column=0)\n",
    "input_K = tk.Entry(bm25_label_frame,width=20)\n",
    "input_K.grid(row=0, column=1)\n",
    "tk.Label(bm25_label_frame, text=\"B\").grid(row=1, column=0)\n",
    "input_B = tk.Entry(bm25_label_frame, width=20)\n",
    "input_B.grid(row=1, column=1)\n",
    "show_bm25_button = tk.Button(bm25_label_frame, text=\"Bm25\", command=show_bm25)\n",
    "show_bm25_button.grid(row=2, column=1)\n",
    "# boolean model\n",
    "boolean_label_frame = ttk.LabelFrame(Matching_label_frame, text=\"Boolean model\")\n",
    "boolean_label_frame.grid(row=1, column=0, padx=10, pady=10)\n",
    "boolean_model_button = tk.Button(boolean_label_frame, text=\"Boolean model\", command=show_boolean_model)\n",
    "boolean_model_button.grid(row=0, column=0)\n",
    "\n",
    "# boolean model\n",
    "Datamining_label_frame = ttk.LabelFrame(Matching_label_frame, text=\"Datamining model\")\n",
    "Datamining_label_frame.grid(row=1, column=1, padx=10, pady=10)\n",
    "Datamining_model_button = tk.Button(Datamining_label_frame, text=\"Datamining model\", command=show_datamining_model)\n",
    "Datamining_model_button.grid(row=0, column=0)\n",
    "\n",
    "for dat in d.TDFP:\n",
    "    table.insert(\"\", \"end\", values=[dat[0],dat[1],dat[2],dat[3]])\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
